[[O’REILLY 핸즈온 LLM 1장 내용정리]]
#### **(슬라이드 1: 제목)**

안녕하세요! 오늘 '핸즈온 LLM' 1장의 내용을 바탕으로 '대규모 언어 모델의 세계'에 대해 소개해 드릴 박중영입니다. 이 시간에는 언어 AI의 기본 개념부터 최신 동향, 그리고 우리가 직접 LLM을 다루는 방법까지 알아보겠습니다.

#### **(슬라이드 2: 언어 AI란?)**

먼저 **언어 AI**가 무엇인지부터 짚고 넘어가겠습니다.

- **인공지능, 즉 AI**는 많이 들어보셨을 겁니다. 음성을 인식하고, 언어를 번역하는 등 **인간의 지능적인 작업을 흉내 내는 컴퓨터 시스템**을 말합니다.
    
- **언어 AI**는 그중에서도 **인간의 언어를 이해하고, 처리하며, 새로운 문장을 만들어내는 기술**에 초점을 맞춘 분야입니다. 흔히 자연어 처리, NLP라고도 불리죠. 이 책에서는 LLM뿐만 아니라 검색 시스템처럼 언어 AI 분야에 큰 영향을 미친 기술까지 포괄하는 넓은 의미로 '언어 AI'라는 용어를 사용합니다.
    

#### **(슬라이드 3: 언어 AI의 최근 역사 - 초기)**

언어 AI는 어떻게 발전해 왔을까요? 그 여정을 짧게 따라가 보겠습니다.

- 초창기에는 **BoW(Bag-of-Words)**, 즉 '단어 가방'이라는 기법이 있었습니다. 텍스트를 단어 주머니처럼 보고, 어떤 단어가 몇 번 나왔는지만 세는 아주 간단한 방식이었죠. 하지만 이 방법은 문장의 의미나 순서를 전혀 고려하지 못하는 명확한 한계가 있었습니다.
    
- 2013년, **word2vec**이 등장하며 큰 변화가 시작됩니다. 이는 단어를 **의미를 담은 벡터, 즉 숫자의 배열로 표현**하는 첫 시도였습니다. 신경망을 통해 문장에서 다음에 올 단어를 예측하며 단어 간의 관계를 학습했죠. 덕분에 '왕'에서 '남자'를 빼고 '여자'를 더하면 '여왕'이 나온다는 식의 의미적 연산이 가능해졌습니다.
    

#### **(슬라이드 4: 언어 AI의 최근 역사 - 트랜스포머의 등장)**

그리고 언어 AI의 역사를 바꾼 게임 체인저가 등장합니다.

- 바로 **어텐션(Attention) 메커니즘**과 **트랜스포머(Transformer)**입니다. "사과가 맛있다"와 "스티브 잡스의 사과"에서 '사과'의 의미가 다른 것처럼, 문맥에 따라 단어의 의미가 달라지는 문제를 해결하기 위해 '어텐션' 개념이 도입되었습니다.
    
- 2017년, 구글의 "Attention Is All You Need"라는 전설적인 논문에서 **트랜스포머**가 발표됩니다. 이 모델은 오직 어텐션 메커니즘만으로 문장 전체를 한 번에 처리하여 **훈련 속도를 획기적으로 높였습니다.** 현재 우리가 사용하는 대부분의 LLM이 바로 이 트랜스포머 구조를 기반으로 하고 있습니다.
    

#### **(슬라이드 5: 언어 AI의 최근 역사 - BERT와 GPT)**

트랜스포머를 기반으로 두 개의 거대한 모델 흐름이 만들어집니다.

- 하나는 **BERT**입니다. BERT는 문장의 빈칸을 맞추는 훈련을 통해 **문맥을 정확하게 이해하는 데 초점**을 맞춘 '표현 모델'입니다. 주로 텍스트를 분류하거나 의미를 분석하는 데 탁월한 성능을 보입니다.
    
- 다른 하나는 우리에게 익숙한 **GPT**입니다. GPT는 다음에 올 단어를 예측하며 **새로운 텍스트를 생성하는 데 특화**된 '생성 모델'입니다. GPT-1부터 GPT-3를 거치며 모델의 크기(파라미터)가 폭발적으로 증가했고, 단순한 텍스트 완성을 넘어 질문에 답하는 '챗봇'의 형태로 발전했습니다.
    

#### **(슬라이드 6: 2023년, 생성 AI의 해)**

그리고 2023년, 생성 AI는 전성기를 맞이합니다.

- **ChatGPT**가 출시되면서 AI 분야에 혁명을 일으켰습니다. 단 5일 만에 100만 명, 두 달 만에 1억 명의 사용자를 모으며 대중화에 성공했죠.
    
- 이후 Llama, Mistral 등 수많은 오픈 소스 및 독점 LLM들이 놀라운 속도로 공개되었습니다. Mamba와 같은 새로운 아키텍처도 등장하며 트랜스포머의 아성에 도전하고 있습니다.
    
- 이때부터 특정 작업을 위해 미세 튜닝될 수 있는 기본 모델을 **파운데이션 모델**이라고 부르기 시작했습니다.
    

#### **(슬라이드 7: '대규모 언어 모델'의 정의)**

그렇다면 우리가 말하는 **'대규모 언어 모델(LLM)'**은 정확히 무엇일까요?

- 보통은 GPT처럼 텍스트를 생성하는 아주 큰 모델을 LLM이라고 생각하는 경향이 있습니다.
    
- 하지만 '대규모'라는 기준은 사실 매우 상대적입니다. 지금의 대규모 모델도 몇 년 뒤에는 소규모로 여겨질 수 있죠.
    
- 그래서 이 책에서는 범위를 넓혀, **텍스트를 직접 생성하지 않는 BERT 같은 표현 모델이나, 심지어 개인 컴퓨터에서 실행할 수 있는 작은 임베딩 모델까지도 '대규모 언어 모델'의 범주에 포함**해서 다룹니다.
    

#### **(슬라이드 8: LLM의 훈련 패러다임)**

LLM은 어떻게 똑똑해질까요? 훈련 방식은 크게 두 단계로 나뉩니다.

1. **사전 훈련 (Pre-training)**: 가장 많은 시간과 자원이 투입되는 단계입니다. 인터넷의 방대한 텍스트를 학습하며 문법, 상식, 언어 패턴 등을 익힙니다. 이렇게 만들어진 모델을 **'베이스 모델'** 또는 **'파운데이션 모델'**이라고 부릅니다. 하지만 이 상태의 모델은 아직 특정 지시를 잘 따르지는 못합니다.
    
2. **미세 튜닝 (Fine-tuning)**: 사전 훈련된 베이스 모델을 특정 작업(예: 고객 리뷰 분석, 법률 문서 요약)에 맞게 추가로 훈련하는 과정입니다. 이를 통해 LLM은 우리가 원하는 구체적인 작업을 수행할 수 있게 됩니다. 사전 훈련보다 훨씬 적은 자원으로 가능해 매우 효율적입니다.
    

#### **(슬라이드 9: LLM, 왜 유용한가?)**

LLM은 왜 이렇게 각광받을까요? 활용 분야가 무궁무진하기 때문입니다.

- **텍스트 분류**: 고객 리뷰가 긍정적인지 부정적인지 자동으로 판단할 수 있습니다.
    
- **클러스터링**: 수많은 문의사항 속에서 자주 발생하는 주제를 찾아낼 수 있습니다.
    
- **시맨틱 검색 및 RAG**: 단순 키워드 검색을 넘어, 의미 기반으로 문서를 찾고 그 내용을 바탕으로 답변을 생성할 수 있습니다.
    
- **챗봇 구축**: 외부 도구나 최신 정보를 활용하여 똑똑한 대화형 시스템을 만들 수 있습니다.
    
- **멀티모달 작업**: 이미지나 소리 같은 다른 유형의 데이터를 이해하고, '이 사진으로 요리 레시피를 만들어 줘'와 같은 복합적인 작업도 가능합니다.
    
- 물론, 동화책을 쓰거나 롤플레잉 게임을 하는 등 **창의적인 작업**에도 훌륭하게 활용될 수 있습니다.
    

#### **(슬라이드 10: 책임 있는 LLM 개발과 사용)**

하지만 강력한 기술에는 큰 책임이 따릅니다. LLM을 사용할 때 우리는 몇 가지를 반드시 고려해야 합니다.

- **편향과 공정성**: LLM은 훈련 데이터에 존재하는 편견을 그대로 학습하고, 심지어 증폭시킬 수 있습니다.
    
- **투명성과 책임성**: LLM이 너무 사람처럼 말해서 진짜 사람과 구분이 어려울 수 있고, 사람의 감독 없이 사용될 경우 의료나 금융 분야에서 심각한 문제를 일으킬 수 있습니다.
    
- **유해 콘텐츠**: 가짜 뉴스나 유해한 정보를 생성할 위험이 있습니다.
    
- **지적 재산권**: LLM이 만든 창작물의 저작권은 누구에게 있으며, 훈련 데이터에 저작권이 있는 자료가 사용되지는 않았는지 등의 문제가 있습니다.
    
- **규제**: EU의 AI 법(AI Act)처럼 각국 정부도 LLM 개발과 배포를 규제하려는 움직임을 보이고 있습니다.
    

#### **(슬라이드 11: 자원이 부족해도 괜찮습니다)**

"LLM을 다루려면 엄청 비싼 컴퓨터가 필요하지 않나요?" 라고 생각하실 수 있습니다.

- 맞습니다. LLM 훈련과 실행에는 **고성능 GPU**, 특히 **VRAM**이라는 비디오 메모리가 많이 필요합니다. Llama 2 같은 모델을 훈련하는 데는 수십억 원이 들기도 하죠. 그래서 'GPU 거지'라는 웃픈 신조어까지 생겼습니다.
    
- **하지만 걱정 마세요!** 이 책은 바로 **'GPU 거지'를 위한 책**입니다. 모든 예제는 **구글 코랩(Colab)의 무료 버전**에서도 충분히 실행할 수 있도록 설계되었습니다. 비싼 장비 없이도 LLM의 세계에 입문할 수 있습니다.
    

#### **(슬라이드 12: LLM 인터페이스: 오픈 소스 vs. 비공개)**

LLM을 사용하는 방법은 크게 두 가지입니다.

- **비공개 모델 (Closed Source)**: OpenAI의 GPT-4나 Anthropic의 Claude처럼 모델의 내부 구조나 가중치가 공개되지 않은 모델입니다. 우리는 API를 통해 이 모델들을 사용하며, 복잡한 인프라 걱정 없이 최고의 성능을 경험할 수 있습니다. 다만 비용이 비싸고, 우리 데이터를 외부로 보내야 한다는 단점이 있습니다.
    
- **오픈 모델 (Open Models)**: Meta의 Llama나 Mistral처럼 모델의 모든 것이 공개된 경우입니다. 강력한 하드웨어만 있다면 모델을 직접 다운로드해서 내 컴퓨터에서 실행하고, 원하는 대로 미세 튜닝도 할 수 있습니다. **완전한 제어권과 투명성**이 가장 큰 장점이며, 이 책에서는 주로 오픈 모델을 다룰 예정입니다.
    

#### **(슬라이드 13: 첫 번째 텍스트 생성하기)**

자, 이제 이론은 충분합니다! 직접 텍스트를 생성해 볼까요?

- **허깅 페이스 허브(Hugging Face Hub)**는 LLM의 놀이터와 같은 곳입니다. 여기서 수많은 모델을 찾아보고 다운로드할 수 있습니다.
    
- 이 책에서는 주로 **'Phi-3-mini'** 라는 모델을 사용할 겁니다. 이 모델은 38억 파라미터로 비교적 작지만 성능이 뛰어나고, 상업적으로도 자유롭게 사용할 수 있다는 큰 장점이 있습니다.
    
- 텍스트 생성 과정은 간단합니다. **1) 모델을 로드하고, 2) 텍스트를 토큰으로 변환하는 토크나이저를 로드한 뒤, 3) 모델에 토큰화된 입력을 전달**하면 끝입니다.
    

2장에서는 텍스트를 숫자로 바꾸는 '토큰화'와 '임베딩'에 대해 더 깊이 알아보고, 3장에서는 트랜스포머 아키텍처의 내부 동작 원리를 파헤쳐 볼 예정입니다.