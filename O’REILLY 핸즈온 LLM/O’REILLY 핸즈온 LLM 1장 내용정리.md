
## **1. 언어 AI란?**

- **인공지능 (Artificial Intelligence, AI)**:
    - 음성 인식, 언어 번역, 시각 인식 등 **인간 지능에 가까운 작업을 수행하는 컴퓨터 시스템**을 묘사하는 데 사용되는 용어.
    - 존 매카시(John McCarthy)의 정의: "지능적인 기계, 특히 지능적인 컴퓨터 프로그램을 만드는 과학과 공학".
- **언어 AI (Language AI)**:
    - **인간 언어를 이해하고, 처리하며, 생성할 수 있는 기술을 개발**하는 데 초점을 맞춘 AI의 하위 분야.
    - 종종 자연어 처리(NLP)와 혼용되어 사용됨.
    - 이 책에서는 LLM뿐만 아니라 검색 시스템과 같이 언어 AI 분야에 큰 영향을 미친 기술까지 포괄하기 위해 이 용어를 사용.

---

## **2. 언어 AI의 최근 역사**

- 언어 표현 및 생성을 위한 다양한 알고리즘과 모델들이 등장하며 발전.
- **BoW (Bag-of-Words)**:
    - **비구조적인 텍스트를 수치 표현(벡터)으로 변환**하는 초기 기법.
    - 텍스트를 토큰(단어 또는 부분단어)으로 분할(토큰화)하고, 어휘사전을 만든 후 각 단어의 등장 횟수를 카운트.
    - 단점: 텍스트의 본질적인 의미를 무시하고 단순히 단어의 모음으로만 간주.
- **word2vec (밀집 벡터 임베딩)**:
    - 2013년에 개발된, **텍스트의 의미를 포착하는 임베딩(벡터 표현)**을 만드는 첫 시도.
    - 신경망을 사용하여 주어진 문장에서 다음 단어 예측을 통해 단어 간의 관계를 학습.
    - 단어 임베딩의 값을 통해 단어의 속성을 표현하고 의미적 유사성을 측정 가능.
    - 임베딩에는 단어, 문장, 문서 임베딩 등 다양한 종류가 있음.
- **어텐션(Attention) 메커니즘**:
    - 문맥에 따라 단어의 의미가 달라지는 문제를 해결하기 위해 도입. 순환 신경망(RNN)에서 입력 시퀀스의 특정 부분에 주의를 기울여 문맥 정보를 통합하는 방식.
- **트랜스포머(Transformer)**:
    - 2017년 논문 "Attention Is All You Need"에서 소개. **RNN을 제거하고 어텐션 메커니즘만으로 구성**.
    - **훈련 속도를 크게 높일 수 있는 병렬 훈련**이 가능하게 함.
    - 인코더 블록과 디코더 블록으로 구성되며, **셀프 어텐션**을 통해 시퀀스 내 모든 부분에 주의를 기울여 한 번에 처리 가능.
    - 현재 대부분의 대규모 언어 모델(LLM)의 기반 구조.
- **BERT (인코더 기반 모델)**:
    - 2018년에 소개된 인코더 기반 구조로, **언어 표현에 초점**을 둠.
    - **마스크드 언어 모델링(MLM)** 기법으로 사전 훈련되어 정확한 문맥 표현을 학습.
    - **전이 학습(Transfer Learning)**에 사용되어 사전 훈련 후 텍스트 분류와 같은 특정 작업을 위해 미세 튜닝됨.
    - **표현 모델(Representation Model)**은 주로 언어를 표현하고 임베딩을 생성하며, 텍스트를 직접 생성하지는 않음.
- **GPT (생성 모델, 디코더 기반 모델)**:
    - 2018년에 소개된 디코더 기반 구조로, **텍스트 생성에 초점**을 둠.
    - GPT-1(1.17억), GPT-2(15억), GPT-3(1,750억) 등으로 **지속적으로 파라미터가 증가**.
    - 초기에는 텍스트 완성에 사용되었으나, 질문에 답변하도록 훈련되어 **인스트럭트 모델(Instruct Model) 또는 채팅 모델(Chat Model)**로 발전.
    - **문맥 길이(Context Length)**: LLM이 처리할 수 있는 최대 토큰 수에 제한이 있음.
- **2023년: 생성 AI의 해**:
    - ChatGPT(GPT-3.5)의 출시로 AI 분야에 혁명적 영향, 빠른 사용자 증가 (5일 만에 1백만 명, 2개월 만에 1억 명).
    - 다양한 오픈 소스 및 독점 LLM(예: Llama, Mistral, Phi)이 놀라운 속도로 공개됨.
    - 새로운 아키텍처(Mamba, RWKV)도 등장하여 트랜스포머에 필적하는 성능과 장점을 보여줌.
    - **파운데이션 모델(Foundation Model)**은 명령 수행을 위해 미세 튜닝될 수 있는 오픈 소스 베이스 모델을 의미.

---

## **3. '대규모 언어 모델'의 정의**

- 일반적인 인식:
    - 주로 **디코더 기반의 대규모 생성 모델**을 '대규모 언어 모델(Large Language Model, LLM)'이라 칭하는 경향이 있음.
- **이 책에서의 정의**:
    - **'대규모'라는 용어의 정의는 임의적**이며, 미래에는 현재의 대규모 모델이 소규모로 간주될 수 있음.
    - 이 책에서는 **텍스트를 생성하지 않고 사용자 하드웨어에서 실행할 수 있는 모델(예: 임베딩 모델, 표현 모델, BoW)**도 '대규모 언어 모델'에 포함하여 다룸.
    - 이는 단순히 규모뿐만 아니라 **다양한 종류의 언어 모델을 포괄**하는 개념으로 이해.

---

## **4. 대규모 언어 모델의 훈련 패러다임**

- **전통적인 머신러닝**:
    - 주로 분류와 같은 특정 작업을 위해 단일 단계로 모델을 훈련.
- **LLM 훈련의 다단계 방식**:
    
    1. **언어 모델링 (사전 훈련, Pre-training)**:
        - 대부분의 계산 및 훈련 시간이 소요되는 첫 단계.
        - 인터넷에서 수집한 대규모 텍스트 말뭉치에서 훈련하여 문법, 맥락, 언어 패턴을 학습 (주로 다음 토큰 예측).
        - 이 단계에서 만들어진 모델을 **베이스 모델(Base Model) 또는 파운데이션 모델(Foundation Model)**이라 부름.
        - 이 모델은 일반적으로 특정 명령을 따르지 못함.
    2. **미세 튜닝 (Fine-tuning, 사후 훈련 Post-training)**:
        - 사전 훈련된 모델을 사용하여 **구체적인 작업에 맞춰 추가 훈련**.
        - 이를 통해 LLM이 특정 작업에 적응하거나 원하는 행동을 수행할 수 있게 됨 (예: 분류, 명령 수행).
        - 사전 훈련 단계에 비해 자원 소모가 적어 비용 효율적.
    
    - **미세 튜닝된 모델 또한 사전 훈련된 모델의 일종으로 간주**.

---

## **5. 대규모 언어 모델 애플리케이션: 왜 유용한가요?**

- LLM의 특성은 광범위한 작업에 잘 맞으며, 텍스트 생성과 프롬프트를 통해 상상력이 유일한 제약인 것처럼 보임.
- **주요 사용 사례 및 기술**:
    - **텍스트 분류**: 고객 리뷰의 긍정/부정 식별 (지도 학습).
    - **텍스트 클러스터링 및 토픽 모델링**: 이슈 티켓에서 자주 발생하는 주제 찾기 (비지도 학습).
    - **시맨틱 검색(Semantic Search) 및 RAG(Retrieval-Augmented Generation)**: 관련 문서 검색 및 조사, 외부 정보 활용.
    - **LLM 챗봇 구축**: 도구 및 문서와 같은 외부 자원을 활용하여 복잡한 시스템 구현.
    - **멀티모달(Multimodal) 작업**: 이미지 기반으로 요리 레시피 작성 등 다른 유형의 입력(비전) 적용.
- LLM은 롤플레잉, 동화책 쓰기와 같은 **창의적인 작업**에서도 점점 더 즐겁게 활용됨.

---

## **6. 책임 있는 LLM 개발과 사용**

- LLM의 영향력이 커짐에 따라 **사회적, 윤리적 의미**를 염두에 두는 것이 중요.
- **핵심 고려 사항**:
    - **편향과 공정성**: LLM은 대규모 훈련 데이터에 포함된 편향을 학습하고 재현하며 증폭시킬 수 있음. 훈련 데이터가 잘 공유되지 않아 어떤 편향이 내재하는지 알기 어려움.
    - **투명성과 책임성**: LLM이 사람처럼 대화할 수 있어 사람과의 구분이 어려움. 사람의 감독 없는 LLM 사용은 의도치 않은 결과를 초래할 수 있음 (예: 의료 분야 규제).
    - **유해 콘텐츠 생성**: 잘못된 텍스트, 가짜 뉴스, 오해의 소지가 있는 정보 등을 생성할 가능성.
    - **지적 재산권**: LLM 출력의 소유권 및 훈련 데이터에 저작권 있는 자료 사용 여부 문제.
    - **규제**: EU 인공지능법(EU AI Act)과 같이 LLM 개발 및 배포를 규제하는 움직임이 시작됨.

---

## **7. 자원이 부족해도 괜찮습니다**

- **컴퓨팅 자원**:
    - 생성 모델 실행에는 일반적으로 **GPU(그래픽 카드)**가 많이 필요하며, 특히 **VRAM(Video Random-Access Memory)**의 양이 중요.
    - LLM 훈련 및 미세 튜닝은 매우 많은 비용을 소모 (예: Llama 2 훈련에 5백만 달러 이상).
    - **'GPU 거지(GPU-geoji)'**라는 용어는 비싼 GPU나 많은 예산 없이 대규모 모델을 훈련하기 어려운 현실을 반영.
- **이 책의 접근 방식**:
    - **GPU 거지를 위한 책**: 비싼 GPU 없이도 실행할 수 있는 모델을 사용.
    - 모든 예제는 **구글 코랩(Google Colab)의 무료 인스턴스(16GB VRAM을 가진 T4 GPU)**에서 실행할 수 있도록 작성. 이는 이 책에서 필요한 최소 VRAM 용량에 해당.

---

## **8. 대규모 언어 모델 인터페이스**

- LLM을 사용하고 내부 동작을 이해하는 데 중요한 요소.
- **독점 및 비공개 모델(Closed Source Models)**:
    - 가중치와 구조가 대중에게 공개되지 않음 (예: OpenAI의 GPT-4, Anthropic의 Claude).
    - API(Application Programming Interface)를 통해 접근. 공급자가 모델 호스팅 및 실행을 관리하여 사용자에게 고성능 GPU가 불필요.
    - 장점: 뛰어난 성능, 낮은 진입 장벽.
    - 단점: 높은 서비스 비용, 미세 튜닝 불가, 데이터 공유 필요.
- **오픈 모델(Open Models)**:
    - 가중치와 구조가 대중에게 공개됨 (예: Cohere Command R, Mistral, Microsoft Phi, Meta Llama).
    - 강력한 GPU가 있다면 로컬 컴퓨터에서 모델을 직접 다운로드하여 사용 가능.
    - 장점: **완전한 제어, 미세 튜닝 가능, 투명성**, 허깅 페이스(Hugging Face)와 같은 커뮤니티의 지원.
    - 단점: 강력한 하드웨어 및 설정 지식 필요.
    - 이 책에서는 투명성 등의 이점으로 **오픈 소스 모델을 선호**.
- **오픈 소스 프레임워크**:
    - LLM을 활용하는 다양한 패키지 및 프레임워크 존재 (예: llama.cpp, LangChain, Hugging Face Transformers).
    - 이 책은 특정 프레임워크에 국한되지 않고 LLM 활용의 안정적인 방법을 제시하며, 직관적인 이해를 강조.

---

## **9. 첫 번째 텍스트 생성하기**

- **LLM 선택 및 로드**:
    - LLM을 찾아보고 다운로드하기 좋은 곳은 **허깅 페이스 허브(Hugging Face Hub)**.
    - 이 책에서는 주로 **Phi-3-mini** 모델을 사용.
        - **비교적 작고(38억 파라미터) 성능이 높음**.
        - 8GB VRAM 이하 장치에서 실행 가능하며, 양자화(quantization) 시 6GB VRAM보다 적은 장치에서도 사용 가능.
        - **MIT 라이선스를 가지고 있어 상업적 목적으로 사용하는 데 제약이 없음**.
    - 새로운 LLM이 자주 출시되므로, 대부분의 예제는 어떤 LLM과도 동작하도록 작성.
- **텍스트 생성 과정**:
    1. **생성 모델 자체 로드**.
    2. **모델이 사용하는 토크나이저(Tokenizer) 로드**.
    3. 토크나이저는 입력 텍스트를 생성 모델에 주입하기 전에 토큰으로 분할.
    4. 모델에 토큰화된 입력을 전달하여 텍스트 생성.

---

## **10. 1장 요약**

- **LLM의 혁신적인 영향**:
    - 언어 AI 분야에 혁명을 가져왔으며, 번역, 분류, 요약 등 다양한 작업을 해결하는 방식을 근본적으로 변화.
- **언어 AI 역사의 핵심 개념**:
    - 간단한 BoW 표현부터 신경망을 사용한 복잡한 표현까지 발전.
    - **문맥 인코딩을 위한 어텐션(Attention) 메커니즘**이 LLM 성능 실현의 핵심 요소.
- **LLM의 두 가지 주요 유형**:
    - **표현 모델 (인코더 기반 모델, 예: BERT)**: 주로 언어를 표현하고 임베딩을 생성.
    - **생성 모델 (디코더 기반 모델, 예: GPT 모델 패밀리)**: 주로 텍스트를 생성.
    - **이 책에서는 두 종류의 모델 모두를 대규모 언어 모델로 간주**.
- **LLM의 광범위한 측면**:
    - 애플리케이션, 사회적 및 윤리적 의미, 그리고 이런 모델을 실행하는 데 필요한 자원 등을 소개.
- **첫 번째 텍스트 생성 경험**: Phi-3 모델을 사용하여 텍스트 생성 실습.
- **다음 장 예고**:
    - 2장에서는 토큰화와 임베딩에 대해 자세히 다루며, 3장에서는 트랜스포머 아키텍처를 통한 LLM의 작동 방식을 탐구할 예정.

---