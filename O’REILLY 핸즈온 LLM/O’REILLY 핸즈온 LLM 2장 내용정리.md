2장 '토큰과 임베딩'에서는 대규모 언어 모델(LLM)의 핵심 개념인 **토큰화**와 **임베딩**에 대한 내용

- **LLM 토큰화 (LLM Tokenization)**
    
    - **토크나이저의 역할**: LLM은 한 번에 하나의 토큰씩 출력을 생성하며, 입력 텍스트도 모델에 전달되기 전에 토크나이저를 통해 토큰으로 분할됩니다. 토크나이저는 또한 모델의 출력 토큰 ID를 단어나 토큰으로 변환하는 역할도 합니다.
    - **토큰 분할 방식 결정 요인**: 토크나이저가 텍스트를 분할하는 방법은 **토큰화 방법 (BPE, WordPiece 등)**, **어휘사전 크기 및 특수 토큰(예: `[CLS]`, `[SEP]`, `[UNK]`, `<|user|>`, `<|assistant|>`)과 같은 토크나이저 설계 선택**, 그리고 **훈련 데이터셋의 도메인**이라는 세 가지 주요 요소에 의해 결정됩니다.
    - **토큰의 종류**:
        - **단어 토큰**: 초기 토큰화에 사용되었으나, 새로운 단어 처리나 유사 단어의 중복 문제로 인해 현대 NLP에서는 사용이 줄고 있습니다.
        - **부분단어 토큰**: 현재 가장 널리 사용되는 방법으로, 완전한 단어와 부분단어를 모두 포함하며, 새로운 단어도 작은 단위로 분할하여 표현할 수 있습니다.
        - **문자 토큰**: 새로운 단어 처리에 유용하지만, 모델링이 어렵고 문맥 길이에 비효율적일 수 있습니다.
        - **바이트 토큰**: 유니코드 문자를 바이트로 분할하는 방법으로, 다국어 환경에서 경쟁력이 있다고 언급됩니다.
    - **주요 LLM 토크나이저 비교**: BERT, GPT-2, Flan-T5, GPT-4, StarCoder2, Galactica, Phi-3와 같은 다양한 LLM의 토크나이저가 대문자 처리, 비영어권 언어, 이모지, 프로그래밍 코드, 특수 토큰 처리 방식에서 어떻게 다른지 비교합니다. 예를 들어, GPT-4 토크나이저는 여러 공백을 하나의 토큰으로 처리하거나 프로그래밍 키워드를 단일 토큰으로 표현하여 코드 생성에 특화된 경향을 보입니다.
- **토큰 임베딩 (Token Embeddings)**
    
    - **임베딩의 개념**: LLM은 토크나이저 어휘사전의 각 토큰에 연관된 **임베딩 벡터**를 가집니다. 이 벡터는 훈련 과정에서 업데이트되어 유용한 값을 할당받습니다.
    - **문맥을 고려한 단어 임베딩**: 언어 모델은 원시적인 정적 토큰 임베딩을 넘어서 **문맥을 고려한 토큰 임베딩**을 생성합니다. 이는 단어가 문맥에 따라 다른 임베딩으로 표현될 수 있게 하여 개체명 인식, 추출적 텍스트 요약, AI 이미지 생성 시스템(DALL-E, Midjourney, Stable Diffusion) 등 다양한 애플리케이션에 활용됩니다.
- **텍스트 임베딩 (문장과 전체 문서) (Text Embeddings (Sentences and Whole Documents))**
    
    - 토큰보다 긴 텍스트(문장, 문단, 전체 문서)를 하나의 벡터로 표현하는 **텍스트 임베딩 모델**이 존재합니다. 이는 텍스트의 의미를 포착하며, **시맨틱 검색**이나 **토픽 모델링**과 같은 애플리케이션에 필수적입니다.
- **LLM을 넘어 활용되는 단어 임베딩 (Word Embeddings Beyond LLMs)**
    
    - **Word2vec**: LLM 이전에는 word2vec, GloVe, fastText와 같은 단어 임베딩이 널리 사용되었습니다. Word2vec은 **슬라이딩 윈도우**를 사용하여 문맥 내에서 단어 쌍이 이웃에 나타날 가능성을 예측하도록 신경망을 훈련함으로써 단어의 의미를 포착합니다. 이 과정의 핵심 아이디어는 **스킵그램**과 **네거티브 샘플링**입니다.
- **추천 시스템을 위한 임베딩 (Embeddings for Recommendation Systems)**
    
    - 임베딩은 추천 시스템에도 유용하게 사용됩니다. 예를 들어, word2vec 알고리즘을 활용하여 재생목록에 있는 노래들을 임베딩하면, 의미적으로 유사한 노래들을 추천할 수 있습니다.

2장은 LLM이 텍스트를 처리하는 데 필수적인 **토큰화**의 다양한 방법과 특징을 설명하고, 텍스트의 의미를 수치적으로 표현하는 **임베딩**의 중요성을 강조합니다. 특히 문맥을 고려한 임베딩이 어떻게 LLM의 능력과 다양한 AI 애플리케이션을 가능하게 하는지 자세히 다룹니다.