2장 '토큰과 임베딩'에서는 대규모 언어 모델(LLM)의 핵심 개념인 **토큰화**와 **임베딩**에 대한 내용

- **LLM 토큰화 (LLM Tokenization)**
    
    - **토크나이저의 역할**: LLM은 한 번에 하나의 토큰씩 출력을 생성하며, 입력 텍스트도 모델에 전달되기 전에 토크나이저를 통해 토큰으로 분할됩니다. 토크나이저는 또한 모델의 출력 토큰 ID를 단어나 토큰으로 변환하는 역할도 합니다.
    - **토큰 분할 방식 결정 요인**: 토크나이저가 텍스트를 분할하는 방법은 **토큰화 방법 (BPE, WordPiece 등)**, **어휘사전 크기 및 특수 토큰(예: `[CLS]`, `[SEP]`, `[UNK]`, `<|user|>`, `<|assistant|>`)과 같은 토크나이저 설계 선택**, 그리고 **훈련 데이터셋의 도메인**이라는 세 가지 주요 요소에 의해 결정됩니다.
    - **토큰의 종류**:
        - **단어 토큰**: 초기 토큰화에 사용되었으나, 새로운 단어 처리나 유사 단어의 중복 문제로 인해 현대 NLP에서는 사용이 줄고 있습니다.
        - **부분단어 토큰**: 현재 가장 널리 사용되는 방법으로, 완전한 단어와 부분단어를 모두 포함하며, 새로운 단어도 작은 단위로 분할하여 표현할 수 있습니다.
        - **문자 토큰**: 새로운 단어 처리에 유용하지만, 모델링이 어렵고 문맥 길이에 비효율적일 수 있습니다.
        - **바이트 토큰**: 유니코드 문자를 바이트로 분할하는 방법으로, 다국어 환경에서 경쟁력이 있다고 언급됩니다.
    - **주요 LLM 토크나이저 비교**: BERT, GPT-2, Flan-T5, GPT-4, StarCoder2, Galactica, Phi-3와 같은 다양한 LLM의 토크나이저가 대문자 처리, 비영어권 언어, 이모지, 프로그래밍 코드, 특수 토큰 처리 방식에서 어떻게 다른지 비교합니다. 예를 들어, GPT-4 토크나이저는 여러 공백을 하나의 토큰으로 처리하거나 프로그래밍 키워드를 단일 토큰으로 표현하여 코드 생성에 특화된 경향을 보입니다.
- **토큰 임베딩 (Token Embeddings)**
    
    - **임베딩의 개념**: LLM은 토크나이저 어휘사전의 각 토큰에 연관된 **임베딩 벡터**를 가집니다. 이 벡터는 훈련 과정에서 업데이트되어 유용한 값을 할당받습니다.
    - **문맥을 고려한 단어 임베딩**: 언어 모델은 원시적인 정적 토큰 임베딩을 넘어서 **문맥을 고려한 토큰 임베딩**을 생성합니다. 이는 단어가 문맥에 따라 다른 임베딩으로 표현될 수 있게 하여 개체명 인식, 추출적 텍스트 요약, AI 이미지 생성 시스템(DALL-E, Midjourney, Stable Diffusion) 등 다양한 애플리케이션에 활용됩니다.
- **텍스트 임베딩 (문장과 전체 문서) (Text Embeddings (Sentences and Whole Documents))**
    
    - 토큰보다 긴 텍스트(문장, 문단, 전체 문서)를 하나의 벡터로 표현하는 **텍스트 임베딩 모델**이 존재합니다. 이는 텍스트의 의미를 포착하며, **시맨틱 검색**이나 **토픽 모델링**과 같은 애플리케이션에 필수적입니다.
- **LLM을 넘어 활용되는 단어 임베딩 (Word Embeddings Beyond LLMs)**
    
    - **Word2vec**: LLM 이전에는 word2vec, GloVe, fastText와 같은 단어 임베딩이 널리 사용되었습니다. Word2vec은 **슬라이딩 윈도우**를 사용하여 문맥 내에서 단어 쌍이 이웃에 나타날 가능성을 예측하도록 신경망을 훈련함으로써 단어의 의미를 포착합니다. 이 과정의 핵심 아이디어는 **스킵그램**과 **네거티브 샘플링**입니다.
- **추천 시스템을 위한 임베딩 (Embeddings for Recommendation Systems)**
    
    - 임베딩은 추천 시스템에도 유용하게 사용됩니다. 예를 들어, word2vec 알고리즘을 활용하여 재생목록에 있는 노래들을 임베딩하면, 의미적으로 유사한 노래들을 추천할 수 있습니다.

2장은 LLM이 텍스트를 처리하는 데 필수적인 **토큰화**의 다양한 방법과 특징을 설명하고, 텍스트의 의미를 수치적으로 표현하는 **임베딩**의 중요성을 강조합니다. 특히 문맥을 고려한 임베딩이 어떻게 LLM의 능력과 다양한 AI 애플리케이션을 가능하게 하는지 자세히 다룹니다.

# LLM의 핵심: 토큰과 임베딩 - 요점정리

## **1. 2장 소개: LLM의 기본 구성 요소**

- **핵심 개념**: **토큰(Token)**과 **임베딩(Embedding)**은 대규모 언어 모델(LLM)의 중심 개념임.
- **중요성**: 이 두 가지를 이해해야 LLM의 작동 원리와 구축 방법을 명확히 파악할 수 있음.
- **학습 내용**: 토큰의 정의와 사용법, 그리고 임베딩을 통한 언어의 수치적 표현 방법을 다룸.
    

## **2. LLM 토큰화 (LLM Tokenization)**

- **토큰(Token)**
    - LLM의 기본 **입력/출력 단위**
    - 모델은 텍스트를 토큰으로 분해하여 인식하고, 생성할 때도 한 번에 하나의 토큰을 만듦.
- **토크나이저(Tokenizer) 역할**
    - 모델의 입력을 준비 (텍스트 → 토큰 ID).
    - 모델의 출력을 변환 (토큰 ID → 텍스트).
        
- **토큰화 결정 요인**
- 
    - **토큰화 방법**: 모델별로 다름
        - **BPE**: GPT 계열
        - **WordPiece**: BERT
        - **SentencePiece**: Flan-T5 계열
            
    - **토크나이저 설계**:
        - **어휘사전 크기**: 3만, 5만, 10만 개 이상 등.
        - **특수 토큰**: 문장의 시작/끝, 알 수 없는 단어, 마스킹 등을 나타내는 특수 기호.
        - **대소문자 처리**: 구분 여부 (cased vs. uncased).
        - 
    - **훈련 데이터셋**: 동일한 방법이라도 훈련 데이터(영어, 코드, 다국어 등)에 따라 성능이 달라짐.
        
- **주요 토큰화 유형**
    - **단어 토큰**: 공백 기준 분할. 신조어 처리에 약함.
    - **부분단어(Subword) 토큰**: 단어와 부분단어를 혼합. 신조어 표현이 가능해 **LLM에서 가장 널리 사용됨**.
    - **문자 토큰**: 개별 문자로 분할. 모델링이 어려움.
    - **바이트 토큰**: 유니코드 문자를 바이트로 분할. 다국어 환경에 유리.
        
- **주요 LLM 토크나이저 비교**
    - **BERT**: WordPiece 사용. `[CLS]`, `[SEP]` 등 특수 토큰 활용.
    - **GPT-2/4**: BPE 사용. 대소문자와 줄바꿈을 유지하며, 특히 GPT-4는 코드 표현에 최적화됨 (예: `elif`를 단일 토큰으로 처리).
    - **Flan-T5**: SentencePiece 사용. 이모지, 한자 등은 `<unk>`(알 수 없음) 토큰으로 처리.
    - **StarCoder2**: 코드 생성에 특화. 여러 공백이나 숫자를 각각의 토큰으로 인코딩.
    - **Galactica**: 과학 지식에 특화. 인용, 수학, 화학식 등을 위한 특수 토큰 사용.
    - **Phi-3**: 채팅에 특화. `<|user|>`와 `<|assistant|>` 같은 화자 역할 토큰 추가.

## **3. 토큰 임베딩 (Token Embeddings)**

- **임베딩(Embedding)**
    - 언어를 모델이 이해할 수 있도록 **수치(벡터)로 표현**한 것.
    - 단어의 의미와 패턴을 수치 공간에 나타냄.
- **LLM의 임베딩 활용**
    - 토크나이저의 어휘사전에 있는 모든 토큰은 고유한 임베딩 벡터를 가짐.
    - 이 벡터 값들은 모델 훈련 과정에서 의미 있는 값으로 업데이트됨.
- **문맥을 고려한 단어 임베딩**
    - LLM은 **문맥에 따라 동일한 단어도 다른 임베딩 벡터로 표현**함. (예: '사과'가 과일인지, 기업인지 구분)
    - 이를 통해 텍스트 분류, 요약 등 다양한 작업이 가능해짐.
    - DALL-E 같은 AI 이미지 생성 시스템의 기반 기술이기도 함.
        

## **4. 텍스트 임베딩 (Text Embeddings)**

- **목적**: 문장, 문단 등 긴 텍스트를 **하나의 벡터로 표현**해야 할 때 사용.
- **방법**: 텍스트 내 모든 토큰 임베딩의 **평균값**을 계산하는 방식이 널리 쓰임.
- **라이브러리**: `sentence-transformers`가 주로 사용됨.
- **활용 분야**: 시맨틱 검색(의미 기반 검색), 토픽 모델링 등.
    

## **5. word2vec: LLM 외부의 단어 임베딩**

- **개요**: 2013년 개발된 초기 단어 임베딩 모델로, 텍스트의 의미를 벡터로 포착하는 데 성공한 첫 사례.
- **훈련 방식**
    - **중심 단어**와 주변 **이웃 단어** 쌍으로 학습 데이터를 구성.
    - 두 단어가 같은 문맥에 등장할 확률을 예측하도록 신경망을 훈련.
    - **네거티브 샘플링**: 이웃이 아닌 단어들을 학습에 포함시켜 모델 성능을 높임.
- **결과**: 비슷한 문맥에서 자주 등장하는 단어들은 서로 가까운 임베딩 벡터 값을 갖게 됨.
    

## **6. 추천 시스템을 위한 임베딩**

- **개념 확장**: 임베딩은 텍스트를 넘어 **추천 시스템**에서도 널리 활용됨.
- **예시 (음악 추천)**
    - 노래를 '단어'로, 재생목록을 '문장'으로 간주.
    - word2vec 알고리즘으로 각 노래의 임베딩을 생성.
    - 임베딩 공간에서 가까운 노래, 즉 재생목록에 함께 자주 등장하는 노래를 추천.
## **7. 결론**

- **핵심 정리**: 2장에서는 LLM의 기초인 **토큰화**와 **임베딩**의 원리를 다룸.
- **주요 내용**: 다양한 토크나이저의 특징과 텍스트가 수치로 변환되는 과정을 설명.
- **응용**: word2vec, 추천 시스템 등 LLM 외부에서의 임베딩 활용 사례를 소개.
- **의의**: 이러한 기본 지식은 시맨틱 검색 등 고급 LLM 애플리케이션 구축의 필수 기반이 됨.