3장 '대규모 언어 모델 자세히 살펴보기'는 트랜스포머 언어 모델의 작동 방식을 직관적으로 이해할 수 있게 설명함. LLM의 핵심 구성 요소와 최신 아키텍처의 발전 사항을 다룸.

**LLM의 텍스트 생성 방식:**

- 트랜스포머 LLM은 텍스트를 한 번에 하나의 토큰씩 생성함.
    
- 생성된 각 토큰은 입력 프롬프트 끝에 추가되어 다음 생성 단계의 프롬프트로 사용됨. 이 과정은 모델을 통한 **한 번의 정방향 계산(forward pass)**에 해당함.
    

**주요 구성 요소:**

- 트랜스포머 LLM은 세 가지 핵심 구성 요소로 이루어져 있음:
    
    - **토크나이저(Tokenizer):** 텍스트를 토큰 ID 시퀀스로 변환하며, 모델은 토크나이저의 어휘사전과 연관된 **토큰 임베딩(token embedding)**을 가짐.
        
    - **트랜스포머 블록 스택(Stack of Transformer Blocks):** LLM의 대부분의 계산이 이곳에서 이루어짐. 각 블록은 입력을 처리하여 다음 블록으로 전달함.
        
    - **언어 모델링 헤드(LM Head):** 트랜스포머 스택의 출력을 가장 가능성 있는 다음 토큰에 대한 확률로 변환함.
        

**토큰 선택 (샘플링/디코딩):**

- 모델이 출력하는 확률 분포에서 하나의 토큰을 선택하는 방법을 **디코딩 전략(decoding strategy)**이라고 함.
    
- **탐욕적 디코딩(greedy decoding)**은 항상 확률 점수가 가장 높은 토큰을 선택하는 방식이며, LLM의 온도(temperature) 매개변수를 0으로 설정할 때 사용됨.
    
- 일반적으로는 약간의 무작위성을 가미하여 확률이 낮은 단어가 선택될 가능성을 허용하는 샘플링 방식을 사용하면 더 다양한 출력을 얻을 수 있음.
    

**병렬 토큰 처리 및 문맥 크기:**

- 트랜스포머는 뛰어난 병렬 처리 능력을 가지며, 각 토큰을 독립적인 계산 경로 또는 스트림으로 처리함.
    
- 모델이 동시에 처리할 수 있는 토큰 수에는 **문맥 길이(context length)**라는 제한이 있음. 이는 모델이 다룰 수 있는 최대 토큰 수를 나타냄.
    
- 키와 값 캐싱(KV cache) 기술은 이전 계산 결과를 캐싱하여 텍스트 생성 속도를 크게 높임.
    

**트랜스포머 블록 내부:**

- 트랜스포머 블록은 두 가지 주요 구성 요소로 이루어짐:
    
    - **어텐션 층(Attention layer):** 다른 입력 토큰과 위치에 대한 정보를 통합하여 언어의 뉘앙스를 포착함. **셀프 어텐션(self-attention)**은 한 시퀀스 안의 다른 위치에 주의를 기울여 입력 시퀀스를 더 정확하게 표현함.
        
    - 어텐션 계산은 쿼리(Query), 키(Key), 값(Value) 행렬을 사용하여 관련성 점수를 계산하고 정보를 통합하는 두 단계로 수행됨.
        
    - **멀티 헤드 어텐션(multi-head attention)**은 여러 어텐션 메커니즘을 병렬로 실행하여 모델이 복잡한 패턴을 모델링하는 용량을 증가시킴.
        
    - **피드포워드 신경망(Feedforward neural network):** 모델의 처리 용량 대부분을 담당하며, 정보 저장 및 예측, 훈련된 데이터로부터의 보간(interpolation)을 수행함.
        

**트랜스포머 아키텍처의 최근 발전 사항:**

- **효율적인 어텐션:** 어텐션 연산의 계산 비용을 줄이기 위해 로컬/희소 어텐션(local/sparse attention), 멀티 쿼리 어텐션(multi-query attention), 그룹 쿼리 어텐션(grouped-query attention), 그리고 GPU 메모리 이동을 최적화하는 **플래시 어텐션(FlashAttention)**과 같은 기법들이 개발되었음.
    
- **트랜스포머 블록 개선:** 최신 모델에서는 **잔차 연결(residual connection)**과 **층 정규화(layer normalization)**와 같은 요소들이 개선되어 적용됨.
    
- **위치 임베딩(Positional Embedding):** 토큰의 순서 정보를 모델에 제공하는 중요한 요소임. 특히 **로터리 위치 임베딩(Rotary Positional Embedding, RoPE)**은 상대적인 토큰 위치 정보를 인코딩하며 어텐션 단계에서 적용됨.
